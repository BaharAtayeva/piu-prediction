Day 1



This session focused on developing and comparing two approaches to linear regression: a from-scratch version using NumPy and a high-level version using PyTorch. The experiment used a synthetic dataset to observe convergence behavior and deepen understanding of gradient descent mechanics.



Data Generation: Created 100 synthetic data points following the relation y = 3x + 4 + noise using NumPy. From-Scratch Implementation (NumPy).



Dataset size: 100 points

Learning rate: 0.1

Epochs: 100

Loss function: Mean Squared Error (MSE)

Optimizer: Stochastic Gradient Descent (SGD)



Key Insights: The from-scratch method provided a deeper understanding of weight updates in gradient descent and PyTorch significantly simplified model definition and training processes.



Next Steps: - Extend to multiple input features and evaluate performance.

&nbsp;           - Transition to deeper architectures (Multilayer Perceptrons).





